#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä–∞–∑–ª–∏—á–∏–π –ø—Ä–µ—Å–µ—Ç–æ–≤ –¥–ª—è —Ä–µ–¥—É–∫—Ü–∏–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–µ—Å–µ—Ç —Å —Ñ–∞–±—Ä–∏—á–Ω—ã–º–∏ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
"""

import numpy as np
import json
from collections import OrderedDict
import struct

class PresetDifferentialAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –ø—Ä–µ—Å–µ—Ç–∞–º–∏ –¥–ª—è —Ä–µ–¥—É–∫—Ü–∏–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏"""
    
    def __init__(self, osirus_decoder):
        self.decoder = osirus_decoder
        self.factory_presets = {}
        self.significant_parameters = []
        self.reduced_latent_dimensions = {}
        
    def load_factory_presets(self, factory_sysex_file):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ —Ñ–∞–±—Ä–∏—á–Ω—ã–µ –ø—Ä–µ—Å–µ—Ç—ã –∏–∑ .syx —Ñ–∞–π–ª–∞"""
        
        print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–±—Ä–∏—á–Ω—ã—Ö –ø—Ä–µ—Å–µ—Ç–æ–≤ –∏–∑ {factory_sysex_file}...")
        
        try:
            with open(factory_sysex_file, 'rb') as f:
                sysex_data = f.read()
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö SysEx —Å–æ–æ–±—â–µ–Ω–∏–π
            presets = self.parse_multiple_sysex(sysex_data)
            
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(presets)} —Ñ–∞–±—Ä–∏—á–Ω—ã—Ö –ø—Ä–µ—Å–µ—Ç–æ–≤")
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π –ø—Ä–µ—Å–µ—Ç
            for i, preset_sysex in enumerate(presets):
                preset_name = self.extract_preset_name(preset_sysex)
                
                if not preset_name:
                    preset_name = f"Factory_Preset_{i+1}"
                
                # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                decoded = self.decoder.decode_sysex_bytes(preset_sysex)
                
                if decoded:
                    self.factory_presets[preset_name] = {
                        'sysex_data': preset_sysex,
                        'parameters': decoded,
                        'preset_index': i
                    }
                    
                    print(f"   {i+1:3d}. {preset_name}")
            
            return len(self.factory_presets)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–±—Ä–∏—á–Ω—ã—Ö –ø—Ä–µ—Å–µ—Ç–æ–≤: {e}")
            return 0
    
    def parse_multiple_sysex(self, sysex_data):
        """–ü–∞—Ä—Å–∏–Ω–≥ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö SysEx —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        
        presets = []
        current_pos = 0
        
        while current_pos < len(sysex_data):
            # –ù–∞–π—Ç–∏ –Ω–∞—á–∞–ª–æ SysEx (0xF0)
            start_pos = sysex_data.find(0xF0, current_pos)
            
            if start_pos == -1:
                break
            
            # –ù–∞–π—Ç–∏ –∫–æ–Ω–µ—Ü SysEx (0xF7)
            end_pos = sysex_data.find(0xF7, start_pos)
            
            if end_pos == -1:
                break
            
            # –ò–∑–≤–ª–µ—á—å SysEx —Å–æ–æ–±—â–µ–Ω–∏–µ
            preset_sysex = sysex_data[start_pos:end_pos + 1]
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —ç—Ç–æ Access Virus SysEx
            if self.is_virus_sysex(preset_sysex):
                presets.append(preset_sysex)
            
            current_pos = end_pos + 1
        
        return presets
    
    def is_virus_sysex(self, sysex_bytes):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —ç—Ç–æ SysEx Access Virus"""
        
        # Access Virus SysEx: F0 00 20 33 01 00
        virus_header = bytes([0xF0, 0x00, 0x20, 0x33, 0x01, 0x00])
        
        return len(sysex_bytes) >= len(virus_header) and sysex_bytes[:len(virus_header)] == virus_header
    
    def extract_preset_name(self, sysex_bytes):
        """–ò–∑–≤–ª–µ—á—å –∏–º—è –ø—Ä–µ—Å–µ—Ç–∞ –∏–∑ SysEx –¥–∞–Ω–Ω—ã—Ö"""
        
        # –ò–º—è –ø—Ä–µ—Å–µ—Ç–∞ –æ–±—ã—á–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ SysEx —Å–æ–æ–±—â–µ–Ω–∏—è
        # –î–ª—è Access Virus C —ç—Ç–æ ASCII —Å—Ç—Ä–æ–∫–∞ –ø–µ—Ä–µ–¥ 0xF7
        
        try:
            # –ü–æ–∏—Å–∫ ASCII —Ç–µ–∫—Å—Ç–∞ –≤ –∫–æ–Ω—Ü–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            name_bytes = []
            
            # –ù–∞—á–∏–Ω–∞–µ–º —Å –∫–æ–Ω—Ü–∞ –∏ –∏—â–µ–º printable ASCII
            for i in range(len(sysex_bytes) - 20, len(sysex_bytes) - 1):
                if i >= 0 and 32 <= sysex_bytes[i] <= 126:  # Printable ASCII
                    name_bytes.append(sysex_bytes[i])
                elif name_bytes:  # –ï—Å–ª–∏ —É–∂–µ —Å–æ–±—Ä–∞–ª–∏ –∏–º—è –∏ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏ non-ASCII
                    break
            
            if name_bytes:
                preset_name = bytes(name_bytes).decode('ascii').strip()
                return preset_name if preset_name else None
            
        except:
            pass
        
        return None
    
    def find_baseline_preset(self, target_name="Contra"):
        """–ù–∞–π—Ç–∏ –±–∞–∑–æ–≤—ã–π –ø—Ä–µ—Å–µ—Ç –ø–æ –∏–º–µ–Ω–∏"""
        
        print(f"üîç –ü–æ–∏—Å–∫ –±–∞–∑–æ–≤–æ–≥–æ –ø—Ä–µ—Å–µ—Ç–∞ '{target_name}'...")
        
        # –ü–æ–∏—Å–∫ –ø–æ —Ç–æ—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
        for preset_name, preset_data in self.factory_presets.items():
            if target_name.lower() in preset_name.lower():
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–∞–∑–æ–≤—ã–π –ø—Ä–µ—Å–µ—Ç: '{preset_name}'")
                return preset_name, preset_data
        
        # –ü–æ–∏—Å–∫ –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
        candidates = []
        for preset_name, preset_data in self.factory_presets.items():
            if any(word in preset_name.lower() for word in target_name.lower().split()):
                candidates.append((preset_name, preset_data))
        
        if candidates:
            print(f"üìã –ù–∞–π–¥–µ–Ω—ã –∫–∞–Ω–¥–∏–¥–∞—Ç—ã:")
            for i, (name, _) in enumerate(candidates):
                print(f"   {i+1}. {name}")
            
            # –í–µ—Ä–Ω—É—Ç—å –ø–µ—Ä–≤—ã–π –∫–∞–Ω–¥–∏–¥–∞—Ç
            return candidates[0]
        
        print(f"‚ùå –ë–∞–∑–æ–≤—ã–π –ø—Ä–µ—Å–µ—Ç '{target_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None, None
    
    def compare_presets(self, user_preset_data, baseline_preset_data, threshold=0.01):
        """–°—Ä–∞–≤–Ω–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–µ—Å–µ—Ç —Å –±–∞–∑–æ–≤—ã–º"""
        
        print(f"üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ—Å–µ—Ç–æ–≤ (–ø–æ—Ä–æ–≥ —Ä–∞–∑–ª–∏—á–∏–π: {threshold})...")
        
        user_params = user_preset_data['parameters']
        baseline_params = baseline_preset_data['parameters']
        
        differences = {}
        significant_changes = []
        
        # –°—Ä–∞–≤–Ω–∏—Ç—å –∫–∞–∂–¥—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
        for param_name in user_params.keys():
            if param_name == 'meta':
                continue
            
            if param_name in baseline_params:
                user_val = user_params[param_name]['normalized_value']
                baseline_val = baseline_params[param_name]['normalized_value']
                
                difference = abs(user_val - baseline_val)
                
                differences[param_name] = {
                    'user_value': user_val,
                    'baseline_value': baseline_val,
                    'difference': difference,
                    'relative_change': difference / (baseline_val + 1e-8),  # –ò–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
                    'category': user_params[param_name]['category'],
                    'cc_number': user_params[param_name].get('cc_number'),
                    'is_significant': difference > threshold
                }
                
                if difference > threshold:
                    significant_changes.append({
                        'parameter': param_name,
                        'category': user_params[param_name]['category'],
                        'difference': difference,
                        'user_value': user_val,
                        'baseline_value': baseline_val,
                        'cc_number': user_params[param_name].get('cc_number')
                    })
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤–µ–ª–∏—á–∏–Ω–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        significant_changes.sort(key=lambda x: x['difference'], reverse=True)
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(significant_changes)} –∑–Ω–∞—á–∏–º—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π")
        
        return differences, significant_changes
    
    def analyze_parameter_importance(self, significant_changes):
        """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        
        print(f"\nüìä –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–ê–†–ê–ú–ï–¢–†–û–í:")
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category_changes = {}
        cc_mapped_changes = []
        
        for change in significant_changes:
            category = change['category']
            
            if category not in category_changes:
                category_changes[category] = []
            
            category_changes[category].append(change)
            
            if change['cc_number']:
                cc_mapped_changes.append(change)
        
        # –í—ã–≤–æ–¥ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for category, changes in category_changes.items():
            print(f"\n   üéõÔ∏è {category.upper()} ({len(changes)} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤):")
            
            for change in changes[:5]:  # –¢–æ–ø 5 –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                cc_info = f" (CC{change['cc_number']})" if change['cc_number'] else ""
                print(f"      {change['parameter']}{cc_info}: {change['baseline_value']:.3f} ‚Üí {change['user_value']:.3f} (Œî{change['difference']:.3f})")
        
        # CC –º–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏)
        if cc_mapped_changes:
            print(f"\n   üéöÔ∏è CC –ê–í–¢–û–ú–ê–¢–ò–ó–ò–†–£–ï–ú–´–ï ({len(cc_mapped_changes)} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤):")
            for change in cc_mapped_changes:
                print(f"      CC{change['cc_number']}: {change['parameter']} (Œî{change['difference']:.3f})")
        
        return category_changes, cc_mapped_changes
    
    def create_reduced_latent_space(self, significant_changes, target_dimensions=[32, 64, 128]):
        """–°–æ–∑–¥–∞—Ç—å —Ä–µ–¥—É—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ"""
        
        print(f"\nüéØ –°–û–ó–î–ê–ù–ò–ï –†–ï–î–£–¶–ò–†–û–í–ê–ù–ù–û–ì–û –õ–ê–¢–ï–ù–¢–ù–û–ì–û –ü–†–û–°–¢–†–ê–ù–°–¢–í–ê:")
        
        # –†–∞–Ω–∂–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        importance_scores = []
        
        for change in significant_changes:
            # –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏
            importance = (
                change['difference'] * 2.0 +  # –í–µ–ª–∏—á–∏–Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è
                (1.0 if change['cc_number'] else 0.0) * 1.5 +  # CC –º–∞–ø–ø–∏–Ω–≥
                (1.0 if change['category'] == 'filter' else 0.0) * 1.2 +  # –§–∏–ª—å—Ç—Ä –≤–∞–∂–µ–Ω –¥–ª—è wobble
                (1.0 if change['category'] == 'lfo' else 0.0) * 1.0 +  # LFO –≤–∞–∂–µ–Ω –¥–ª—è –º–æ–¥—É–ª—è—Ü–∏–∏
                (1.0 if change['category'] == 'effects' else 0.0) * 0.8  # –≠—Ñ—Ñ–µ–∫—Ç—ã –º–µ–Ω–µ–µ –∫—Ä–∏—Ç–∏—á–Ω—ã
            )
            
            importance_scores.append({
                'parameter': change['parameter'],
                'importance': importance,
                'change_data': change
            })
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        importance_scores.sort(key=lambda x: x['importance'], reverse=True)
        
        # –°–æ–∑–¥–∞—Ç—å —Ä–µ–¥—É—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Ä–∞–∑–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        reduced_spaces = {}
        
        for target_dim in target_dimensions:
            selected_params = importance_scores[:target_dim]
            
            reduced_spaces[f'{target_dim}d'] = {
                'dimension': target_dim,
                'selected_parameters': [p['parameter'] for p in selected_params],
                'importance_scores': [p['importance'] for p in selected_params],
                'parameter_details': selected_params,
                'coverage': len(selected_params) / len(significant_changes) if significant_changes else 0
            }
            
            print(f"   üìê {target_dim}D –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ: {len(selected_params)} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ({reduced_spaces[f'{target_dim}d']['coverage']:.1%} –ø–æ–∫—Ä—ã—Ç–∏–µ)")
        
        self.reduced_latent_dimensions = reduced_spaces
        return reduced_spaces
    
    def create_reduced_feature_vectors(self, user_preset_data, reduced_space_config):
        """–°–æ–∑–¥–∞—Ç—å —Ä–µ–¥—É—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        
        user_params = user_preset_data['parameters']
        
        reduced_vectors = {}
        
        for space_name, config in reduced_space_config.items():
            dimension = config['dimension']
            selected_params = config['selected_parameters']
            
            # –ò–∑–≤–ª–µ—á—å –∑–Ω–∞—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            feature_vector = []
            feature_names = []
            
            for param_name in selected_params:
                if param_name in user_params:
                    feature_vector.append(user_params[param_name]['normalized_value'])
                    feature_names.append(param_name)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç—å –¥–æ —Ü–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            while len(feature_vector) < dimension:
                feature_vector.append(0.0)
                feature_names.append(f'padding_{len(feature_names)}')
            
            # –û–±—Ä–µ–∑–∞—Ç—å –µ—Å–ª–∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç
            feature_vector = feature_vector[:dimension]
            feature_names = feature_names[:dimension]
            
            reduced_vectors[space_name] = {
                'vector': np.array(feature_vector),
                'feature_names': feature_names,
                'dimension': dimension,
                'non_zero_features': len([x for x in feature_vector if x > 0.001])
            }
        
        return reduced_vectors
    
    def validate_reduction_quality(self, original_latent_features, reduced_vectors):
        """–û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–¥—É–∫—Ü–∏–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏"""
        
        print(f"\nüìà –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –†–ï–î–£–ö–¶–ò–ò:")
        
        validation_results = {}
        
        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
        orig_384d = original_latent_features['feature_vector_384d']
        orig_512d = original_latent_features['feature_vector_512d']
        
        for space_name, reduced_data in reduced_vectors.items():
            reduced_vector = reduced_data['vector']
            dimension = reduced_data['dimension']
            
            # Metrics
            information_retention = reduced_data['non_zero_features'] / dimension
            compression_ratio = dimension / len(orig_384d)
            
            # Dubstep —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã?
            dubstep_features = original_latent_features['dubstep_features']
            wobble_preserved = any('filter' in name for name in reduced_data['feature_names'][:10])
            lfo_preserved = any('lfo' in name for name in reduced_data['feature_names'][:10])
            
            validation_results[space_name] = {
                'dimension': dimension,
                'information_retention': information_retention,
                'compression_ratio': compression_ratio,
                'dubstep_features_preserved': {
                    'wobble_components': wobble_preserved,
                    'lfo_components': lfo_preserved,
                    'overall_preservation': (wobble_preserved and lfo_preserved)
                },
                'efficiency_score': information_retention / compression_ratio
            }
            
            print(f"   üéØ {space_name.upper()}:")
            print(f"      –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {information_retention:.1%}")
            print(f"      –°–∂–∞—Ç–∏–µ: {compression_ratio:.3f}x")
            print(f"      Wobble —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {'‚úÖ' if wobble_preserved else '‚ùå'}")
            print(f"      LFO —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {'‚úÖ' if lfo_preserved else '‚ùå'}")
            print(f"      –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {validation_results[space_name]['efficiency_score']:.2f}")
        
        return validation_results
    
    def analyze_user_vs_factory_preset(self, user_preset_file, factory_sysex_file, baseline_name="Contra"):
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è vs —Ñ–∞–±—Ä–∏—á–Ω—ã–π –ø—Ä–µ—Å–µ—Ç"""
        
        print(f"üîç –ü–û–õ–ù–´–ô –î–ò–§–§–ï–†–ï–ù–¶–ò–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó")
        print(f"   User preset: {user_preset_file}")
        print(f"   Factory presets: {factory_sysex_file}")
        print(f"   Baseline: {baseline_name}")
        
        # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–±—Ä–∏—á–Ω—ã–µ –ø—Ä–µ—Å–µ—Ç—ã
        factory_count = self.load_factory_presets(factory_sysex_file)
        
        if factory_count == 0:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–±—Ä–∏—á–Ω—ã–µ –ø—Ä–µ—Å–µ—Ç—ã")
            return None
        
        # 2. –ù–∞–π—Ç–∏ –±–∞–∑–æ–≤—ã–π –ø—Ä–µ—Å–µ—Ç
        baseline_name, baseline_data = self.find_baseline_preset(baseline_name)
        
        if not baseline_data:
            print("‚ùå –ë–∞–∑–æ–≤—ã–π –ø—Ä–µ—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None
        
        # 3. –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–µ—Å–µ—Ç
        user_ml_data = self.decoder.analyze_preset_for_ml(user_preset_file)
        
        if not user_ml_data:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–µ—Å–µ—Ç")
            return None
        
        # 4. –°—Ä–∞–≤–Ω–∏—Ç—å –ø—Ä–µ—Å–µ—Ç—ã
        differences, significant_changes = self.compare_presets(
            user_ml_data, baseline_data, threshold=0.01
        )
        
        # 5. –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        category_changes, cc_changes = self.analyze_parameter_importance(significant_changes)
        
        # 6. –°–æ–∑–¥–∞—Ç—å —Ä–µ–¥—É—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
        reduced_spaces = self.create_reduced_latent_space(significant_changes)
        
        # 7. –°–æ–∑–¥–∞—Ç—å —Ä–µ–¥—É—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
        reduced_vectors = self.create_reduced_feature_vectors(user_ml_data, reduced_spaces)
        
        # 8. –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–¥—É–∫—Ü–∏–∏
        validation = self.validate_reduction_quality(
            user_ml_data['latent_features'], reduced_vectors
        )
        
        # –°–æ–±—Ä–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        analysis_results = {
            'user_preset': user_preset_file,
            'baseline_preset': baseline_name,
            'factory_presets_count': factory_count,
            'differences_analysis': {
                'total_parameters': len(differences),
                'significant_changes': len(significant_changes),
                'differences_by_parameter': differences,
                'top_changes': significant_changes
            },
            'importance_analysis': {
                'category_changes': category_changes,
                'cc_mapped_changes': cc_changes
            },
            'reduced_latent_spaces': reduced_spaces,
            'reduced_vectors': {k: {**v, 'vector': v['vector'].tolist()} for k, v in reduced_vectors.items()},
            'validation_results': validation,
            'recommendations': self.generate_recommendations(validation, significant_changes)
        }
        
        return analysis_results
    
    def generate_recommendations(self, validation_results, significant_changes):
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏"""
        
        recommendations = {
            'optimal_dimension': None,
            'reasoning': [],
            'trade_offs': {},
            'ml_training_advice': []
        }
        
        # –ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        best_efficiency = 0
        best_dimension = None
        
        for space_name, metrics in validation_results.items():
            efficiency = metrics['efficiency_score']
            
            if efficiency > best_efficiency and metrics['dubstep_features_preserved']['overall_preservation']:
                best_efficiency = efficiency
                best_dimension = space_name
        
        recommendations['optimal_dimension'] = best_dimension
        
        # –†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
        if len(significant_changes) <= 32:
            recommendations['reasoning'].append("32D –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - –º–∞–ª–æ –∑–Ω–∞—á–∏–º—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π")
        elif len(significant_changes) <= 64:
            recommendations['reasoning'].append("64D —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è - —É–º–µ—Ä–µ–Ω–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å")
        else:
            recommendations['reasoning'].append("128D –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ - –≤—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏–π")
        
        # –°–æ–≤–µ—Ç—ã –¥–ª—è ML
        cc_count = len([c for c in significant_changes if c.get('cc_number')])
        
        if cc_count > 5:
            recommendations['ml_training_advice'].append("–í—ã—Å–æ–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ - —Ñ–æ–∫—É—Å –Ω–∞ CC events")
        
        filter_changes = len([c for c in significant_changes if c['category'] == 'filter'])
        if filter_changes > 2:
            recommendations['ml_training_advice'].append("–ê–∫—Ç–∏–≤–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—è –º–æ–¥—É–ª—è—Ü–∏—è - –∏–¥–µ–∞–ª—å–Ω–æ –¥–ª—è Dubstep")
        
        return recommendations
    
    def save_differential_analysis(self, analysis_results, output_prefix='differential_analysis'):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        
        # –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        with open(f'{output_prefix}_complete.json', 'w') as f:
            json.dump(analysis_results, f, indent=2, default=str)
        
        # –†–µ–¥—É—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
        for space_name, vector_data in analysis_results['reduced_vectors'].items():
            np.save(f'{output_prefix}_{space_name}.npy', np.array(vector_data['vector']))
        
        # –û—Ç—á–µ—Ç —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        with open(f'{output_prefix}_recommendations.json', 'w') as f:
            json.dump(analysis_results['recommendations'], f, indent=2)
        
        print(f"\nüíæ –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω:")
        print(f"   - {output_prefix}_complete.json")
        for space_name in analysis_results['reduced_vectors'].keys():
            print(f"   - {output_prefix}_{space_name}.npy")
        print(f"   - {output_prefix}_recommendations.json")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    
    # –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–µ–∫–æ–¥–µ—Ä Osirus
    from osirus_preset_decoder import OsirusPresetDecoder
    
    # –°–æ–∑–¥–∞—Ç—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    decoder = OsirusPresetDecoder()
    analyzer = PresetDifferentialAnalyzer(decoder)
    
    # –ü—Ä–æ–≤–µ—Å—Ç–∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    results = analyzer.analyze_user_vs_factory_preset(
        user_preset_file='osiris_preset.txt',
        factory_sysex_file='osiris_all_presets.syx',
        baseline_name='Contra'
    )
    
    if results:
        print(f"\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–ò–§–§–ï–†–ï–ù–¶–ò–ê–õ–¨–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê:")
        
        diff_analysis = results['differences_analysis']
        print(f"   üìä –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {diff_analysis['total_parameters']}")
        print(f"   üîÑ –ó–Ω–∞—á–∏–º—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π: {diff_analysis['significant_changes']}")
        
        recommendations = results['recommendations']
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        print(f"   üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {recommendations['optimal_dimension']}")
        
        for reason in recommendations['reasoning']:
            print(f"   üí≠ {reason}")
        
        for advice in recommendations['ml_training_advice']:
            print(f"   ü§ñ {advice}")
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        analyzer.save_differential_analysis(results)
        
        print(f"\n‚úÖ –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"   –†–µ–¥—É—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≥–æ—Ç–æ–≤–æ –¥–ª—è ML –æ–±—É—á–µ–Ω–∏—è")
        
    else:
        print("‚ùå –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è")


if __name__ == "__main__":
    main()